{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4DSrlXs71OE"
   },
   "source": [
    "#  Homework 04: KNN for Churn Prediction\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This week, we will go more deeply into the churn prediction task introduced in the previous homework. As you already know, churn prediction is one of the most popular use cases of machine learning in business, consisting of detecting customers who are likely to cancel a subscription to a service. Churn can be triggered by better price offers, more interesting packages, bad service experiences or personal situation changes experienced by customers. To timely prevent customers' churn, companies might adopt a machine learning classifier able to predict churn on an individual customer basis, and then use the predictions of this classifier to know when countermeasures (e.g., discounts or special offers) against churn are needed, to prevent the churn event.  \n",
    "\n",
    "In the previous homework, you were expected to use a range of demographic and basic contract-related features and create five additional behavioral as well as an appropriate evaluation method and performance metric(s) to assess the goodness of your classifier in predicting churn for customers.\n",
    "\n",
    "In this homework, we ask you again to work on the machine-learning pipeline presented at the end of lecture 4, by instantiating, exploring, and fine-tuning a series of k-Nearest-Neighbors (kNNs) classifiers. Specifically, we will ask you to:\n",
    "\n",
    "- Experiment with distance matrices and KNN classifiers fed with only demographic features or only behavioral features.\n",
    "- Experiment with distance matrices and KNN classifiers fed with both demographic and behavioral features. \n",
    "- Report, visually, the performance of three fine-tuned KNN classifiers (demographic, tuned), (behavioral, tuned), and (combined, tuned), and discuss.\n",
    "\n",
    "The focus of this homework is on modelling and not on the quality of features. Therefore, note that there should be **no** need to perform any feature engineering in this homework. You are expected to re-use the features (and possibly a part of the source code) that you have created in homework 3. If you have not submitted homework 3, please do get in touch with the TAs, and they will provide five example behavioral feature to use in this homework.  \n",
    "\n",
    "\n",
    "## Submission \n",
    "\n",
    "The homework is due **Mar 30, 2021 23:59 CET**. The notebook must be pushed to your GitHub classroom repository. \n",
    "\n",
    "If you have any questions, feel free to use the Q&A forum in Moodle. \n",
    "\n",
    "## Some Instructions \n",
    "\n",
    "1. You are allowed to use any built-in Python library. If you want to use an external library, you have to justify it. \n",
    "2. Make sure you use the data folder provided in the repository in **read-only** mode. \n",
    "3. Please write all your comments in English, and use meaningful variable names in your code. \n",
    "4. Your repo should have a single notebook (plus the required data files) in the master branch. \n",
    "5. Be sure to hand in a fully-run and evaluated notebook (check the rendered notebook on the GitHub website once you have pushed). \n",
    "6. Be sure to provide a textual description of your thoughts, assumptions, solutions, and explanations for your answers, when requested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dJnyk0nS71Ob"
   },
   "source": [
    "## The Data Set\n",
    "\n",
    "In this homework, you will use the same data set as in homework 3. The `data` folder includes CSV files pertaining to customer's demographic attributes, contract information, and monthly service-related data. \n",
    "\n",
    "For the sake of easiness and clarity, in this homework, we will use the convention that:\n",
    "- features you compute from the **customer.csv** file are referred to as **(pseudo)demographic / demographic features**;\n",
    "- features you compute from the **phone_usage.csv**, **services.csv**, and **charges.csv** files are referred to as **behavioral features**.\n",
    "\n",
    "The **target values** are represented by the **churn labels** listed in the **churn.csv** file under the **\"Churn\" column**.   \n",
    "\n",
    "Each file is characterized by the following attributes:\n",
    "\n",
    "**customer.csv**\n",
    "\n",
    "| Name                   | Description                         |\n",
    "| ---------------------- | ------------------------------------------------------------ |\n",
    "| CustomerID | A unique ID that identifies each customer.  | \n",
    "| Gender |  The customer’s gender: Male, Female  | \n",
    "| SeniorCitizen | Indicates if the customer is 65 or older: 0, 1.  | \n",
    "| Partner |  Indicates if the customer is married: Yes, No | \n",
    "| Dependents | Indicates if the customer lives with any dependents: Yes, No.  |  \n",
    "| PaperlessBilling | Indicates if the customer has chosen paperless billing: Yes, No.  | \n",
    "| PaymentMethod |  Indicates how the customer pays their bill: Mailed check, Electronic check, Credit card, Bank transfer.  | \n",
    "\n",
    "**contract.csv**\n",
    "\n",
    "| Name                   | Description                         |\n",
    "| ---------------------- | ------------------------------------------------------------ |\n",
    "| ContractID |  A unique ID that identifies each contract.  | \n",
    "| CustomerID |  A unique ID that identifies each customer.  | \n",
    "| Contract |  Indicates the customer’s current contract type: Month-to-Month, One Year, Two Year.  | \n",
    "| StartDate |  Start date of the contract. | \n",
    "\n",
    "**churn.csv**\n",
    "\n",
    "| Name                   | Description                         |\n",
    "| ---------------------- | ------------------------------------------------------------ |\n",
    "| CustomerID | A unique ID that identifies each customer.   | \n",
    "| Churn | 1 = the customer left the company. 0 = the customer remained with the company.  | \n",
    "\n",
    "**phone_usage.csv**\n",
    "\n",
    "| Name                   | Description                         |\n",
    "| ---------------------- | ------------------------------------------------------------ |\n",
    "| ContractID | A unique ID that identifies each contract.  | \n",
    "| Date | The reference period for the monthly usage indicated by this record for this ContractID.  | \n",
    "| MonthlyUsage | Indicates the customer’s monthly usage for the phone.  | \n",
    "\n",
    "**services.csv**\n",
    "\n",
    "| Name                   | Description                         |\n",
    "| ---------------------- | ------------------------------------------------------------ |\n",
    "| ContractID | A unique ID that identifies each contract.  | \n",
    "| ServiceValue | The specific service value of type Service the contract has. No = the customer does not have that Service.   | \n",
    "| Service | A string label identifying a type of service offered by the company: PhoneService, InternetService, MultipleLines, OnlineSecurity, OnlineBackup, DeviceProtection, TechSupport, StreamingTV, StreamingMovies.   | \n",
    "\n",
    "**charges.csv**\n",
    "\n",
    "| Name                   | Description                         |\n",
    "| ---------------------- | ------------------------------------------------------------ |\n",
    "| ContractID |  A unique ID that identifies each contract. | \n",
    "| Date | The billing date for the monthly usage indicated by this record for this ContractID.   | \n",
    "| Charge | Indicates the contract’s monthly charge.  | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rn4tsOQh71Od"
   },
   "outputs": [],
   "source": [
    "### YOUR IMPORT STATEMENTS HERE (please, do not make any imports elsewhere in the notebook) ###\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets, metrics, model_selection, preprocessing\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "41KVAevJ71Oe"
   },
   "outputs": [],
   "source": [
    "# Files with one record per customer / contract\n",
    "customer = pd.read_csv('./data/customer.csv')\n",
    "contract = pd.read_csv('./data/contract.csv')\n",
    "churn = pd.read_csv('./data/churn.csv')\n",
    "\n",
    "# Files with one record per customer / contract over months\n",
    "phone_usage = pd.read_csv('./data/phone_usage.csv')\n",
    "services = pd.read_csv('./data/services.csv')\n",
    "charges = pd.read_csv('./data/charges.csv')\n",
    "\n",
    "y = pd.read_csv('./data/churn.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RqglmRtY71Oe"
   },
   "source": [
    "<a id=\"section1\"></a>\n",
    "## 1  Experiment with KNN classifiers and demographic features\n",
    "----\n",
    "\n",
    "In this section, you should:\n",
    "1. Prepare a feature matrix with only `demographic features` for each customer (one row per customer, features included in the customer.csv file) and compute a `pair-wise distance matrix` across customers, with an appropriate distance measure.\n",
    "2. To predict the churn target, implement three kNN classifiers and feed them with the created pairwise-distance matrix. Evaluate each kNN classifier by using a `10-fold user-stratified cross-validation` and use `Balanced Accuracy` and `AUC` as performance metrics.\n",
    "3. Then, re-use and extend the implementation provided in Task 1.2 to fine tune the hyper-parameter k of the kNN classifier. \n",
    "4. Report visually `Balanced Accuracy` and `AUC` of (i) the three kNN classifiers implemented in Task 1.2 and (ii) the fine-tuned kNN classifier obtained in Task 1.3, in such a way that they can be easily and appropriately compared. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9dl_gq971Of"
   },
   "source": [
    "<a id=\"section1.1\"></a>\n",
    "### Task 1.1 \n",
    "\n",
    "The k-Nearest-Neighbor (kNN) is one of the simplest algorithms for finding patterns in classification problems. \n",
    "\n",
    "In this task, to prepare the training data to be fed into the kNN classifier, we ask you to:\n",
    "- Compute a feature matrix `XD` with one row per customer and one column per **demographic** feature (i.e., **the features included in the customer.csv file**). The cell `XD[i,j]` represents the value of the demographic feature `j` for customer `i`. The shape of the matrix `XD` should be (n_customers, n_demographic_features). \n",
    "- Compute a pair-wise distance matrix `DD` with one row per customer and one column per customer. The cell `DD[i,j]` represents the pair-wise distance between the features vectors `XD[i]` and `XD[j]` of customer `i` and customer `j`, respectively. The shape of the matrix `DD` should be (n_customers, n_customers). Make sure to pick an appropriate distance measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "HgSbCBlf71Og"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE (please, feel free to add extra cells to solve this task, after this first one) ###\n",
    "\n",
    "X_dem = customer\n",
    "XD = X_dem.replace({'Female' : 1, 'Male' : 0, 'Yes' : 1, 'No' : 0, 'Mailed check' : 0, 'Electronic check' : 1, 'Credit card (automatic)' : 2,\n",
    "       'Bank transfer (automatic)' : 3 })\n",
    "XD = XD.drop({'CustomerID'}, axis = 1)\n",
    "dist = DistanceMetric.get_metric('hamming')\n",
    "DD = dist.pairwise(XD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.66666667 0.66666667 ... 0.5        0.33333333 0.5       ]\n",
      " [0.66666667 0.         0.33333333 ... 0.16666667 0.33333333 0.5       ]\n",
      " [0.66666667 0.33333333 0.         ... 0.16666667 0.66666667 0.5       ]\n",
      " ...\n",
      " [0.5        0.16666667 0.16666667 ... 0.         0.5        0.66666667]\n",
      " [0.33333333 0.33333333 0.66666667 ... 0.5        0.         0.16666667]\n",
      " [0.5        0.5        0.5        ... 0.66666667 0.16666667 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(DD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4OwJ6HV71Og"
   },
   "source": [
    "### 1.1 Please describe and motivate your implementation and justify the choice of distance measure.\n",
    "\n",
    "### ============  YOUR WRITTEN ANSWER HERE ============== \n",
    "\n",
    "In the feature matrix (XD), I decided to change the values of the features to integers in order to be able to use sklearn's distance mettric functions. I used the Hamming distance - calculating for each pair of customers the number of features they are different on (divided by the total number of features) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJ5jtFSL71Oh"
   },
   "source": [
    "<a id=\"section1.2\"></a>\n",
    "### Task 1.2\n",
    "\n",
    "As a first exploration, in this task, we ask you to implement three different kNN classifiers able to predict the churn target, with $k=1, 25, 1000$ respectively . The input data should be the pair-wise distance matrix `DD` computed in Task 1.1. Each kNN classifier should be evaluated through a `10-fold user-stratified cross-validation` and the following metrics should be computed: `Balanced Accuracy` and `AUC`. We expect that you discuss the performance metric scores achieved by the three classifiers. No plotting is needed, it is enough to print the scores in the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "MSJu9ZgD71Oi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1:\n",
      "mean balanced accuracy = 0.547419637265816\n",
      "mean AUC = 0.547419637265816\n",
      "----------------------------------\n",
      "k = 25:\n",
      "mean balanced accuracy = 0.5799687678558445\n",
      "mean AUC = 0.7144550428337622\n",
      "----------------------------------\n",
      "k = 1000:\n",
      "mean balanced accuracy = 0.5304316028931476\n",
      "mean AUC = 0.7286301029272908\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE (please, feel free to add extra cells to solve this task, after this first one) ###\n",
    "ks = [1, 25, 1000]\n",
    "yd = np.ravel(y.drop('CustomerID', axis = 1))\n",
    "accs = {}\n",
    "aucs = {}\n",
    "estimators = {}\n",
    "\n",
    "for k in ks:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, metric = 'precomputed')\n",
    "    results = cross_validate(knn, DD, yd, cv=10, scoring = ['balanced_accuracy', 'roc_auc'])\n",
    "    print(f'k = {k}:')\n",
    "    acc = results['test_balanced_accuracy'].mean()\n",
    "    auc = results['test_roc_auc'].mean()\n",
    "    accs[k] = acc\n",
    "    aucs[k] = auc\n",
    "    estimators[k] = knn\n",
    "    print(f'mean balanced accuracy = {acc}')\n",
    "    print(f'mean AUC = {auc}')\n",
    "    print('----------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dptNLHgl71Oi"
   },
   "source": [
    "### 1.2 Please compare and discuss the performance metric scores achieved by the three classifiers.\n",
    "\n",
    "### ============  YOUR WRITTEN ANSWER HERE ============== \n",
    "\n",
    "- none of them really achieve a good balanced accuracy, all are just barely over 50%\n",
    "- based on the AUC, they are all better than a random classifier (which would have an AUC of 0.5)\n",
    "- I think that only demographic data is not enough for accurate predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lBIraLzb71Oj"
   },
   "source": [
    "<a id=\"section1.3\"></a>\n",
    "### Task 1.3\n",
    "\n",
    "Given that the performance of KNNs is very sensitive to the choice of k (i.e., the number of neighbors), in this task, we ask you to fine-tune the KNN classifier over an appropriate `range of values for k`. To this end, you need to re-use and appropriately extend the implementation provided in Task 1.2 in order to make it possible to fine tune the hyper-parameter k of the kNN classifier. Please use the same method (`10-fold user-stratified cross-validation`) and performance metrics (`Balanced Accuracy` and `AUC`) as in task 1.2 to evaluate the classifier. Again, no plotting is needed, it is enough to print the scores in the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "piC9psw471Ok"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE (please, feel free to add extra cells to solve this task, after this first one) ###\n",
    "ks = np.array([5, 15, 30, 50, 100, 200, 500, 700, 1000])\n",
    "yd = np.ravel(y.drop('CustomerID', axis = 1))\n",
    "accs_temp = {}\n",
    "aucs_temp = {}\n",
    "\n",
    "for k in ks:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, metric = 'precomputed')\n",
    "    results = cross_validate(knn, DD, yd, cv=10, scoring = ['balanced_accuracy', 'roc_auc'], return_estimator = True)\n",
    "    acc = results['test_balanced_accuracy'].mean()\n",
    "    auc = results['test_roc_auc'].mean()\n",
    "    accs_temp[k] = acc\n",
    "    aucs_temp[k] = aucs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = max(list(accs_temp.values()))\n",
    "best_k_acc = get_key(accs_temp, best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_auc = max(list(aucs_temp.keys()))\n",
    "best_k_auc = aucs_temp[best_auc]\n",
    "best_k_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key(dict_, val):\n",
    "    for (k, v) in dict_.items():\n",
    "        if(v == val):\n",
    "            return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs[best_k_acc] = best_acc\n",
    "aucs[best_k_acc] = get_key(aucs_temp, best_k_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_best = {}\n",
    "aucs_best = {}\n",
    "accuracies_best['best_demographic'] = accs_temp[best_k_acc]\n",
    "aucs_best['best_demographic'] = aucs_temp[best_k_auc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZawZVYo71Ol"
   },
   "source": [
    "<a id=\"section1.4\"></a>\n",
    "### Task 1.4\n",
    "\n",
    "As presented in this course, to assess the classifier's goodness, you need to report and communicate the performance of your classifier appropriately. To this end, in this task, we ask you to visually report the performance metrics of (i) the three kNN classifiers implemented in Task 1.2 and (ii) the fine-tuned kNN classifier obtained in Task 1.3. In other words, you should properly visualize the `Balanced Accuracy` and `AUC` scores achieved by the mentioned classifiers (computed in Task 1.2 and Task 1.3), so that the classifiers' performance can be easily compared. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "cgbU6NX571Om"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'AUC')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### YOUR CODE HERE (please, feel free to add extra cells to solve this task, after this first one ###\n",
    "ks = ['1', '25', '1000', '200']\n",
    "plt.figure(figsize = (12, 4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.bar(x = ks, height = list(accs.values()), width = 0.5)\n",
    "plt.title('Accuracy')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.bar(x = ks, height = list(aucs.values()), width = 0.5)\n",
    "plt.title('AUC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n_5Z77Cf71Om"
   },
   "source": [
    "### 1.4 Please describe, interpret, compare, and discuss the obtained results (performance metric scores) of the different classifiers.\n",
    "### ============  YOUR WRITTEN ANSWER HERE ============== \n",
    "\n",
    "- k = 200 has the highest accuracy, but it's not much better than the other models\n",
    "- k = 25, k = 1000, and k = 200 all have a very similair AUC value\n",
    "    - all the classifiers are better than a random classifier\n",
    "- when tuning the k parameter, I chose k = 200 as the best value, as it had the highest accuracy, and though its AUC wasn't the highest, the differences between the values are quite small and the AUC for k = 200 was ok."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G1gz70rG71On"
   },
   "source": [
    "<a id=\"section2\"></a>\n",
    "## 2  Experiment with KNN classifiers and demographic plus behavioral features \n",
    "----\n",
    "\n",
    "In this section, you should:\n",
    "\n",
    "1. Prepare a feature matrix with only `behavioral features` for each customer (one row per customer, same behavioral features included in the phone_usage.csv, charges.csv, and/or services.csv files and that you created in the previous homework) and compute a `pair-wise distance matrix` across customers, by appropriately selecting (combining) one (or more if necessary) appropriate distance measure(s).  \n",
    "2. Implement a kNN classifier with hyperparameter finetuning (i.e. finetune k over an appropriate range) and feed it with the created pairwise-distance matrix (the one that refers to only behavioral features). Evaluate the kNN classifier by using a `10-fold user-stratified cross-validation` and use `Balanced Accuracy` and `AUC` as performance metrics.\n",
    "3. Create a pair-wise distance matrix by appropriately combining the pair-wise distance matrices you computed for only demographic features in Task 1.1 and only behavioral features in Task 2.1. Then, again prepare a finetuned kNN classifier and feed it with the combined distance matrix to predict churn. You are asked to again evaluate it using a `10-fold user-stratified cross-validation` and `Balanced Accuracy` and `AUC` as performance metrics.  \n",
    "4. Report visually `Balanced Accuracy` and `AUC` of (i) the fine-tuned kNN classifier obtained in Task 1.3 (demographic, fine-tuned), (ii) the fine-tuned kNN classifier obtained in Task 2.2 (behavioral, fine-tuned), and (iii) the fine-tuned kNN classifier obtained in Task 2.3 (demographic+behavioral, fine-tuned) in such a way that they can be easily and appropriately compared. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nXYut9oH71Oo"
   },
   "source": [
    "<a id=\"section2.1\"></a>\n",
    "### Task 2.1\n",
    "\n",
    "As you might be observed, demographic features do not reveal how customers actually used the services, and adding behavioral features derived from the services the customers subscribed to, the extent to which they are charged monthly, and the actual phone usage, can help in improving the performance of our classifier. Therefore, in this task, we ask you to:\n",
    "- Compute a feature matrix `XB` with one row per customer and one column per **behavioral** feature (i.e., **the behavioral features that you created in homework 3**). The cell `XB[i,j]` represents the value of the behavioral feature `j` for customer `i`. The shape of the matrix `XB` should be (n_customers, n_behavioral_features). \n",
    "- Compute a pair-wise distance matrix `DB` with one row per customer and one column per customer. The cell `DB[i,j]` represents the pair-wise distance between the features vectors `XB[i]` and `XB[j]` of customer `i` and customer `j`, respectively. The shape of the matrix `DB` should be (n_customers, n_customers). Please make sure to choose an appropriate distance measure. \n",
    "\n",
    "One important point to be addressed here is that, depending on the different nature (e.g., numerical, categorical) of the behavioral features you created in the previous homework and you are expected to use here, selecting a single distance measure to create your pair-wise distance matrix might not be the right way to move forward. If the types of your behavioral features are very different, a more advanced strategy is needed. Specifically, you might need to use different distance measures for different subsets of your behavioral features. \n",
    "To support you with this, we provide a concrete example not related to this task for illustration. In this example, we assume that we have five features for a user: height (f1), abdomen circumference (f2), favorite three movies (f3,f4, and f5). Given these features, it is appropriate to use Euclidean distance for the vector [f1,f2] and use Jaccard distance for [f3,f4,f5]. To manage this situation, given a pair `(i, j)` of users, the following strategy can be used:\n",
    "- Compute the Euclidean distance for feature vectors [f1,f2] of users i and j - we assume to denote that by `t1[i,j]`. \n",
    "- Compute the Jaccard distance for [f3,f4,f5] of users i and j - we assume to denote that by `t2[i,j]`. \n",
    "- Compute the final distance as `DB[i,j] = t1[i,j] + t2[i,j]` (do not forget to check the scaling of t1 and t2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tBeUZRfo71Op"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE (please, feel free to add extra cells to solve this task, after this first one) ###\n",
    "\n",
    "XB = contract\n",
    "\n",
    "services_num = services.replace({'Yes' : 1, 'No' : 0, 'DSL' : 0, 'Fiber optic' : 0, 'No phone service' : 0,\n",
    "       'No internet service' : 0})\n",
    "services_num = services_num.groupby('ContractID').sum()\n",
    "XB = XB.merge(right = services_num, on = 'ContractID', how = 'outer').rename({'ServiceValue' : 'NumOfServices'}, axis = 1)\n",
    "\n",
    "# add average monthly usage as feature\n",
    "\n",
    "usage_avg = phone_usage.groupby('ContractID').mean()\n",
    "XB = XB.merge(right = usage_avg, on = 'ContractID', how = 'outer').rename({'MonthlyUsage' : 'AvgMonthlyUsage'}, axis = 1)\n",
    "\n",
    "# add average monthly charges as feature\n",
    "\n",
    "charges_avg = charges.groupby('ContractID').mean()\n",
    "XB = XB.merge(right = charges_avg, on = 'ContractID', how = 'outer').rename({'Charge' : 'AvgMonthlyCharge'}, axis = 1)\n",
    "\n",
    "# add phone service value (yes/no) as feature\n",
    "\n",
    "phone_service = services.loc[services['Service'] == 'PhoneService']\n",
    "XB = (XB.merge(right = phone_service, on = 'ContractID', how = 'outer').drop('Service', axis = 1)).rename({'ServiceValue' : 'PhoneService'}, axis = 1)\n",
    "\n",
    "# add internet service type as feature\n",
    "\n",
    "internet_service = services.loc[services['Service'] == 'InternetService']\n",
    "XB = ((XB.merge(right = internet_service, on = 'ContractID', how = 'outer')).drop('Service', axis = 1)).rename({'ServiceValue' : 'InternetServiceValue'}, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XB = XB.drop(['ContractID', 'CustomerID', 'StartDate'], axis = 1)\n",
    "XB = XB.replace({'Month-to-month' : 0, 'Two year' : 1, 'One year' : 2, 'Yes' : 1, 'No' : 0, 'DSL' : 1, 'Fiber optic' : 2})\n",
    "XB = XB.replace({np.nan : 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XB_cat = XB[['Contract', 'NumOfServices', 'PhoneService', 'InternetServiceValue']]\n",
    "dist = DistanceMetric.get_metric('hamming')\n",
    "DB_cat = dist.pairwise(XB_cat)\n",
    "DB_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XB_cont = XB[['AvgMonthlyUsage', 'AvgMonthlyCharge']]\n",
    "dist = DistanceMetric.get_metric('manhattan')\n",
    "DB_cont = dist.pairwise(XB_cont)\n",
    "scaler = preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "scaler.fit(DB_cont)\n",
    "DB_cont = scaler.transform(DB_cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB = np.add(DB_cat, DB_cont)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zvyNBUjz71Oq"
   },
   "source": [
    "### 2.1 Please, describe, motivate, and discuss all your design choices.\n",
    "\n",
    "### ============  YOUR WRITTEN ANSWER HERE ============== \n",
    "\n",
    "- I used the behavioural features I extracted in the previous homework, and processed them in the same way (converting to integer vals, replacing nans)\n",
    "- I used the Hamming distance for the categorical features, which gave results between 0 and 1\n",
    "- I used the Euclidean distance for the real-valued features, and then scaled them so that they would be in the range (0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c7ktbe4U71Oq"
   },
   "source": [
    "<a id=\"section2.2\"></a>\n",
    "### Task 2.2\n",
    "\n",
    "In this task, we ask you to implement a fine-tuned kNN classifier, using the pair-wise distance matrix `DB` derived from behavioral features, as an input. You can re-use and appropriately adjust the implementation you provided in task 1.3 to perform the same task but on the pair-wise distance matrix derived from demographic features. We ask you to use the again a `10-fold user-stratified cross-validation` and to compute `Balanced Accuracy` and `AUC` for this setting. No plotting is needed, it is enough to print the scores in the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "8jT_GdRF71Oq"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE (please, feel free to add extra cells to solve this task, after this first one) ###\n",
    "ks = np.array([5, 15, 30, 50, 100, 200, 500, 700, 1000])\n",
    "yd = np.ravel(y.drop('CustomerID', axis = 1))\n",
    "accs_temp = {}\n",
    "aucs_temp = {}\n",
    "estimators = {}\n",
    "\n",
    "for k in ks:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, metric = 'precomputed')\n",
    "    results = cross_validate(knn, DB, yd, cv=10, scoring = ['balanced_accuracy', 'roc_auc'])\n",
    "    acc = results['test_balanced_accuracy'].mean()\n",
    "    auc = results['test_roc_auc'].mean()\n",
    "    accs_temp[k] = acc\n",
    "    aucs_temp[k] = auc\n",
    "    estimators[k] = knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{5: 0.5011728854314346,\n",
       " 15: 0.49799915050328875,\n",
       " 30: 0.4994938719907431,\n",
       " 50: 0.5,\n",
       " 100: 0.5,\n",
       " 200: 0.5,\n",
       " 500: 0.5,\n",
       " 700: 0.5,\n",
       " 1000: 0.5}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_acc = max(list(accs_temp.values()))\n",
    "best_k_acc = get_key(accs_temp, best_acc)\n",
    "best_k_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4930309108751095"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_auc = max(list(aucs_temp.values()))\n",
    "best_k_auc = get_key(aucs_temp, best_auc)\n",
    "best_k_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{5: 0.48958338307391747,\n",
       " 15: 0.49011147386591025,\n",
       " 30: 0.4873167971597002,\n",
       " 50: 0.4799653054785479,\n",
       " 100: 0.48757606375316753,\n",
       " 200: 0.4958974890301714,\n",
       " 500: 0.4905698607589534,\n",
       " 700: 0.48731281272232385,\n",
       " 1000: 0.4930309108751095}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aucs_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_best['best_behavioural'] = accs_temp[best_k_acc]\n",
    "aucs_best['best_behavioural'] = aucs_temp[best_k_auc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NCu9c4CP71Or"
   },
   "source": [
    "<a id=\"section2.3\"></a>\n",
    "### Task 2.3\n",
    "\n",
    "Once you reach this point, you have considered demographic features and behavioral features, separately. In this task, we ask you to use the pair-wise distance matrices `DD` and `DB`, obtained for only demographic features and only behavioral features respectively, to appropriately create a single pair-wise distance matrix `D` that considers both demographic and behavioral features jointly. To obtain this pair-wise distance matrix `D`, compute `D[i,j] = DD[i,j] + DB[i,j]`, for each `(i,j)` pair and do not forget to take into account the scaling of DD and DB.\n",
    "\n",
    "Finally, we ask you to implement a fine-tuned kNN classifier, using the pair-wise distance matrix `D` as an input. You can again re-use and appropriately adjust the implementation you provided in Tasks 1.3 and 2.2 to perform the same task but on the pair-wise distance matrix `D` derived from both demographic and behavioral features. Again, you should use a `10-fold user-stratified cross-validation` and compute `Balanced Accuracy` and `AUC` for this setting. It is fine to just print the results in the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "OwAAvlWR71Or"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE (please, feel free to add extra cells to solve this task, after this first one) ###\n",
    "D = np.add(DB, DD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs_temp = {}\n",
    "aucs_temp = {}\n",
    "estimators = {}\n",
    "\n",
    "for k in ks:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, metric = 'precomputed')\n",
    "    results = cross_validate(knn, D, yd, cv=10, scoring = ['balanced_accuracy', 'roc_auc'])\n",
    "    acc = results['test_balanced_accuracy'].mean()\n",
    "    auc = results['test_roc_auc'].mean()\n",
    "    accs_temp[k] = acc\n",
    "    aucs_temp[k] = auc\n",
    "    estimators[k] = knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.568182993898606\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_acc = max(list(accs_temp.values()))\n",
    "best_k_acc = get_key(accs_temp, best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.726485599939817"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_auc = max(list(aucs_temp.keys()))\n",
    "best_k_auc = get_key(aucs_temp, best_auc)\n",
    "best_k_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_best['best_db'] = accs_temp[best_k_acc]\n",
    "aucs_best['best_db'] = aucs_temp[best_k_auc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xbaa_ue971Or"
   },
   "source": [
    "<a id=\"section2.2\"></a>\n",
    "### Task 2.4\n",
    "\n",
    "Once you reach this point, you should have:\n",
    "1. The fine-tuned kNN classifier obtained in Task 1.3 (demographic, fine-tuned).\n",
    "2. The fine-tuned kNN classifier obtained in Task 2.2 (behavioral, fine-tuned).\n",
    "3. The fine-tuned kNN classifier obtained in Task 2.3 (demographic+behavioral, fine-tuned).  \n",
    "\n",
    "In this last task, we ask you to visually report the performance of the three above kNN classifiers. Basically, you should again properly visualize the `Balanced Accuracy` and `AUC` scores achieved by the mentioned classifiers in the 10-fold user-stratified cross validation performed in the respective tasks (Task 1.3, Task 2.2, and Task 2.3), so that the classifiers can be easily compared. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qQn_s76471Os"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE (please, feel free to add extra cells to solve this task, after this first one) ###\n",
    "plt.figure(figsize = (12, 4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.bar(x = best_accuracies.keys(), height = list(best_accuracies.values()), width = 0.5)\n",
    "plt.title('Accuracy')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.bar(x = best_aucs.keys(), height = list(best_aucs.values()), width = 0.5)\n",
    "plt.title('AUC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-WX-QxP71Os"
   },
   "source": [
    "### 2.4 Please describe, interpret, compare, and discuss the obtained results (performance metric scores) of the different classifiers.\n",
    "\n",
    "### ============  YOUR WRITTEN ANSWER HERE ============== \n",
    "\n",
    "the classifiers don't seem to be working well, but I don't understand why - perhaps it's something to do with the scaling ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4ee7eC_71Os"
   },
   "source": [
    "## Congratulations\n",
    "\n",
    "Congratulations! Please, carefully revise your solution and push it when you feel that it is ok. \n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Homework04-KNN_Churn_Prediction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
